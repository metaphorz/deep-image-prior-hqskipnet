{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Guided Deep Image Prior\n",
        "Author: Daniel Russell ([@danielrussruss](https://twitter.com/danielrussruss))\n",
        "\n",
        "This notebook uses [OpenAI's CLIP](https://github.com/openai/CLIP) model to optimize the weights of the [Deep Image Prior](https://github.com/DmitryUlyanov/deep-image-prior) skip network to output an image that matches a text prompt. \n",
        "\n",
        "This is a somewhat minimal implementation intended for research and artistic exploration. Do not use this project or a derivative for commercial work.\n",
        "\n",
        "This notebook would not be possible without the foundational work established by [Ryan Murdock](https://twitter.com/advadnoun) and [Katherine Crowson](https://twitter.com/rivershavewings).\n",
        "\n",
        "Some minor additions: P. Fishwick 01/28/2022\n",
        "1. Merged Katherine Crowson's deep_image_prior into Daniel Russell's original notebook : https://github.com/crowsonkb/deep-image-prior\n",
        "2. Mount Google Drive to save the directory deep_image_prior\n",
        "3. Updated to CLIP model RN50x64 with size 448\n",
        "4. Lowered cutn to 10 for a V100 (16GB memory) - update for an A100\n",
        "5. Iterates over num_images to create an image batch\n",
        "6. Saves the image at each display interval"
      ],
      "metadata": {
        "id": "32ksbekHxZl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Citations and Licenses"
      ],
      "metadata": {
        "id": "2t_EfgIPKqc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "@misc{radford2021learning,\n",
        "      title={Learning Transferable Visual Models From Natural Language Supervision}, \n",
        "      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n",
        "      year={2021},\n",
        "      eprint={2103.00020},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CV}\n",
        "}\n",
        "\n",
        "@article{UlyanovVL17,\n",
        "    author    = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},\n",
        "    title     = {Deep Image Prior},\n",
        "    journal   = {arXiv:1711.10925},\n",
        "    year      = {2017}\n",
        "}\n",
        "```\n",
        "\n",
        "### CLIP\n",
        "```\n",
        "MIT License\n",
        "Copyright (c) 2021 OpenAI\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "```\n",
        "\n",
        "### Deep Image Prior\n",
        "```\n",
        "Copyright 2018 Dmitry Ulyanov\n",
        "\"Please contact me if you want to use this software in a commercial application.\" - Dmitry Ulyanov\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "```"
      ],
      "metadata": {
        "id": "lyXuhf3bK1EL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import libraries"
      ],
      "metadata": {
        "id": "tbLWTWB_FmNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IVlROsSn3P8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View GPU details:\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "84tQr1vQy2O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "_dYJHriA4P_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone only the first time to get deep-image-prior into gdrive\n",
        "# Then comment out\n",
        "!git clone https://github.com/metaphorz/deep-image-prior-hqskipnet\n",
        "!pip install kornia einops git+https://github.com/openai/clip madgrad"
      ],
      "metadata": {
        "id": "Zrjcb_rRy_2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "NdOppuQOxVgR"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./deep-image-prior-hqskipnet')\n",
        "from models import *\n",
        "from utils.sr_utils import *\n",
        "import clip\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "from IPython import display\n",
        "import cv2\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as T\n",
        "import kornia.augmentation as K\n",
        "from einops import rearrange\n",
        "from madgrad import MADGRAD\n",
        "import imageio\n",
        "import random\n",
        "import math\n",
        "\n",
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87RLY8khxVgU"
      },
      "source": [
        "# Load CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GzODxwinxVgV"
      },
      "outputs": [],
      "source": [
        "# clip_model = clip.load('ViT-B/16', device=device)[0]\n",
        "# clip_model = clip.load('ViT-L/14', device=device)[0]\n",
        "clip_model = clip.load('RN50x64', device=device)[0]\n",
        "clip_model = clip_model.eval().requires_grad_(False)\n",
        "#clip_size = 224\n",
        "clip_size = 448\n",
        "clip_normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "class MakeCutouts(torch.nn.Module):\n",
        "    def __init__(self, cut_size, cutn):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.augs = T.Compose([\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            K.RandomAffine(degrees=15, translate=0.1, p=0.8, padding_mode='border', resample='bilinear'),\n",
        "            K.RandomPerspective(0.4, p=0.7, resample='bilinear'),\n",
        "            K.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.7),\n",
        "            K.RandomGrayscale(p=0.15),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        if sideY != sideX:\n",
        "            input = K.RandomAffine(degrees=0, shear=10, p=0.5)(input)\n",
        "\n",
        "        max_size = min(sideX, sideY)\n",
        "        cutouts = []\n",
        "        for cn in range(self.cutn):\n",
        "            if cn > self.cutn - self.cutn//4:\n",
        "                cutout = input\n",
        "            else:\n",
        "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
        "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "                offsety = torch.randint(0, sideY - size + 1, ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        cutouts = torch.cat(cutouts)\n",
        "        cutouts = self.augs(cutouts)\n",
        "        return cutouts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization loop"
      ],
      "metadata": {
        "id": "1_39CKmTIR68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "WWL7PGGtxVgW"
      },
      "outputs": [],
      "source": [
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "def optimize_network(num_iterations, optimizer_type, lr):\n",
        "    global itt\n",
        "    itt = 0\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        random.seed(seed)\n",
        "\n",
        "    make_cutouts = MakeCutouts(clip_size, cutn)\n",
        "\n",
        "    # Initialize DIP skip network\n",
        "    input_depth = 3\n",
        "    # use Katherine Crowson's skip net\n",
        "    from models import get_hq_skip_net\n",
        "    net = get_hq_skip_net(input_depth).to(device)\n",
        "    #net = get_net(\n",
        "    #    input_depth, 'skip',\n",
        "    #    pad='reflection',\n",
        "    #    skip_n33d=128, skip_n33u=128,\n",
        "    #    skip_n11=4, num_scales=7,\n",
        "    #   upsample_mode='bilinear',\n",
        "    #).to(device)\n",
        "\n",
        "    # Initialize input noise\n",
        "    net_input = torch.zeros([1, input_depth, sideY, sideX], device=device).normal_().div(10).detach()\n",
        "\n",
        "    # Encode text prompt with CLIP\n",
        "    target_embed = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
        "    if optimizer_type == 'Adam':\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr)\n",
        "    elif optimizer_type == 'MADGRAD':\n",
        "        optimizer = MADGRAD(net.parameters(), lr, weight_decay=0.01, momentum=0.9)\n",
        "\n",
        "    try:\n",
        "        for _ in range(num_iterations):\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "    \n",
        "            with torch.cuda.amp.autocast():\n",
        "              out = net(net_input).float()\n",
        "            cutouts = make_cutouts(out)\n",
        "            image_embeds = clip_model.encode_image(clip_normalize(cutouts))\n",
        "            loss = spherical_dist_loss(image_embeds, target_embed).mean()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            itt += 1\n",
        "\n",
        "            if itt % display_rate == 0 or save_progress_video:\n",
        "                with torch.inference_mode():\n",
        "                    image = TF.to_pil_image(out[0].clamp(0, 1))\n",
        "                    if itt % display_rate == 0:\n",
        "                        display.clear_output(wait=True)\n",
        "                        display.display(image)\n",
        "                        # added to save image at each display rate - PF\n",
        "                        image.save(f'dip_{timestring}_{itt}.png', quality=100)\n",
        "                        if display_augs:\n",
        "                            aug_grid = torchvision.utils.make_grid(cutouts, nrow=math.ceil(math.sqrt(cutn)))    \n",
        "                    if save_progress_video and itt > 15:\n",
        "                        video_writer.append_data(np.asarray(image))\n",
        "\n",
        "            if anneal_lr:\n",
        "                optimizer.param_groups[0]['lr'] = max(0.00001, .99 * optimizer.param_groups[0]['lr'])\n",
        "\n",
        "            print(f'Iteration {itt} of {num_iterations}')\n",
        "            \n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    finally:\n",
        "        return TF.to_pil_image(net(net_input)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pmx90a2xVgX"
      },
      "source": [
        "# Settings / Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "scrolled": false,
        "id": "iIsWKH9VxVgY"
      },
      "outputs": [],
      "source": [
        "seed = random.randint(0, 2**32)\n",
        "opt_type = 'MADGRAD' # Adam, MADGRAD\n",
        "lr = 0.0025 # learning rate\n",
        "anneal_lr = True # True == lower the learning rate over time\n",
        "\n",
        "# sideX, sideY = 256, 256 # Resolution\n",
        "# sideX, sizeY = 384, 384\n",
        "# sideX, sizeY = 512, 256\n",
        "# sideX, sizeY = 512, 384\n",
        "sideX, sideY = 512,512\n",
        "num_iterations = 350 # More can be better, but there are diminishing returns\n",
        "# cutn of 10 fits in a V100 -PF\n",
        "cutn = 10 # Number of crops of image shown to CLIP, this can affect quality\n",
        "\n",
        "# prompt = 'a moody painting of a lonely duckling'\n",
        "# prompt below from @RiversHaveWings\n",
        "prompt = 'the clockwork angel of [water/fire/earth/air] by Gerardo Dottori'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KStRsOWgxVga"
      },
      "outputs": [],
      "source": [
        "display_rate = 50 # How often the output is displayed and saved. -PF\n",
        "# If you grab a P100 GPU or better, you'll likely want to set this further apart, like >=20.\n",
        "# On T4 and K80, the process is slower so you might want to set a faster display_rate (lower number, towards 1-5).\n",
        "\n",
        "save_progress_video = False\n",
        "display_augs = False # Display grid of augmented image, for debugging\n",
        "\n",
        "# number of images you want to display and save - PF\n",
        "num_images = 4\n",
        "for i in range(0,num_images):\n",
        "  timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "  if save_progress_video:\n",
        "      video_writer = imageio.get_writer(f'dip_{timestring}.mp4', mode='I', fps=30, codec='libx264', quality=7, pixelformat='yuv420p')\n",
        "\n",
        "  # Begin optimization / generation\n",
        "  out = optimize_network(num_iterations, opt_type, lr)\n",
        "  # Save final frame and video to a file\n",
        "  out.save(f'dip_{timestring}.png', quality=100)\n",
        "  if save_progress_video:\n",
        "     video_writer.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "CLIP+Deep Image Prior_HQSkipNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2t_EfgIPKqc9"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}